# -*- coding: utf-8 -*-
"""DQNIIT_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t4Y7wEKuXQkU66wjCuolb8rpoVMZmCMP
"""

import numpy as np
import random
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import RMSprop
from collections import deque
from tensorflow.keras.losses import mean_squared_error

class MECEnv:
    def __init__(self, num_clients, F, E_max, R, alpha_n, alpha_e, alpha_L):
        self.num_clients = num_clients
        self.F = F  # Max CPU-cycle frequency
        self.E_max = E_max  # Max energy units
        self.R = R  # Max wireless bandwidth
        self.alpha_n = alpha_n
        self.alpha_e = alpha_e
        self.alpha_L = alpha_L

        # Define initial state (each client has [fk, ek, rk])
        self.state = np.array([[np.random.uniform(0, F),
                                np.random.uniform(0, E_max),
                                np.random.uniform(0, R)]
                               for _ in range(num_clients)])

        # Define the action space (each client can either participate or not)
        self.action_space = [0, 1]

        # Define whether the environment is done
        self.done = False

    def reset(self):
        # Reset the state to the initial state
        self.state = np.array([[np.random.uniform(0, self.F),
                                np.random.uniform(0, self.E_max),
                                np.random.uniform(0, self.R)]
                               for _ in range(self.num_clients)])
        self.done = False
        return self.state

    def step(self, actions):
        rewards = []
        for idx, action in enumerate(actions):
            if action == 1:
                # Client participates: consume resources and get a reward
                self.state[idx][0] -= np.random.uniform(0, self.state[idx][0])
                self.state[idx][1] -= np.random.uniform(0, self.state[idx][1])
                self.state[idx][2] -= np.random.uniform(0, self.state[idx][2])
                reward = self.compute_reward(actions)
            else:
                # Client does not participate: no reward
                reward = 0
            rewards.append(reward)

        self.done = self.check_done()
        return self.state, rewards, self.done

    def compute_reward(self, actions):
        # Compute the reward based on the given formula
        m = np.sum(actions)
        n = self.num_clients
        E = np.sum(self.state[:, 1])
        E_max = self.E_max * self.num_clients
        L = np.sum(self.state[:, 2])
        L_max = self.R * self.num_clients

        reward = (self.alpha_n * (m / n)) - (self.alpha_e * (E / E_max)) - (self.alpha_L * (L / L_max))
        return reward

    def check_done(self):
        # Check if the episode is done (e.g., all resources are depleted)
        for client_state in self.state:
            if np.any(client_state > 0):
                return False
        return True

    def sample_action(self):
        # Randomly sample an action for each client
        return [np.random.choice(self.action_space) for _ in range(self.num_clients)]

    def render(self):
        # Implement visualization logic
        print(f"State: {self.state}")


class DeepQLearning:

    def __init__(self, env, gamma, epsilon, numberEpisodes):
        self.env = env
        self.gamma = gamma
        self.epsilon = epsilon
        self.numberEpisodes = numberEpisodes

        self.stateDimension = env.state.shape[1] * env.num_clients
        self.actionDimension = 2 ** env.num_clients  # Each client has 2 actions (participate or not)

        self.replayBufferSize = 300
        self.batchReplayBufferSize = 100
        self.updateTargetNetworkPeriod = 100
        self.counterUpdateTargetNetwork = 0
        self.sumRewardsEpisode = []
        self.replayBuffer = deque(maxlen=self.replayBufferSize)
        self.mainNetwork = self.createNetwork()
        self.targetNetwork = self.createNetwork()
        self.targetNetwork.set_weights(self.mainNetwork.get_weights())
        self.actionsAppend = []
        self.best_actions = None
        self.max_reward = -np.inf

    def my_loss_fn(self, y_true, y_pred):
        actions = tf.cast(y_true[:, -1], tf.int32)
        y_true = y_true[:, :-1]
        indices = tf.stack([tf.range(tf.shape(y_true)[0]), actions], axis=1)
        gathered_y_true = tf.gather_nd(y_true, indices)
        gathered_y_pred = tf.gather_nd(y_pred, indices)
        loss = mean_squared_error(gathered_y_true, gathered_y_pred)
        return loss

    def createNetwork(self):
        model = Sequential()
        model.add(Dense(256, input_dim=self.stateDimension, activation='relu'))
        model.add(Dense(128, activation='relu'))
        model.add(Dense(self.actionDimension, activation='linear'))
        model.compile(optimizer=RMSprop(), loss=self.my_loss_fn, metrics=['accuracy'])
        return model

    def trainingEpisodes(self):
        for indexEpisode in range(self.numberEpisodes):
            rewardsEpisode = []
            actions_episode = []
            print("Simulating episode {}".format(indexEpisode))
            print(f"Starting episode {indexEpisode + 1}/{self.numberEpisodes}")
            currentState = self.env.reset().flatten()
            terminalState = False
            while not terminalState:
                action = self.selectAction(currentState, indexEpisode)
                action_binary = [int(x) for x in list(format(action, '0{}b'.format(self.env.num_clients)))]
                actions_episode.append(action_binary)
                nextState, reward, terminalState = self.env.step(action_binary)
                rewardsEpisode.append(np.sum(reward))
                nextState = nextState.flatten()
                self.replayBuffer.append((currentState, action, np.sum(reward), nextState, terminalState))
                self.trainNetwork()
                currentState = nextState
            total_reward = np.sum(rewardsEpisode)
            print("Sum of rewards {}".format(total_reward))
            print(f"Episode {indexEpisode + 1} completed. Sum of rewards: {total_reward}")
            self.sumRewardsEpisode.append(total_reward)

            # Track the best actions based on the highest reward
            if total_reward > self.max_reward:
                self.max_reward = total_reward
                self.best_actions = actions_episode

    def selectAction(self, state, index):
        if index < 1:
            return np.random.choice(self.actionDimension)
        randomNumber = np.random.random()
        if index > 200:
            self.epsilon = 0.999 * self.epsilon
        if randomNumber < self.epsilon:
            return np.random.choice(self.actionDimension)
        else:
            Qvalues = self.mainNetwork.predict(state.reshape(1, -1))
            return np.random.choice(np.where(Qvalues[0, :] == np.max(Qvalues[0, :]))[0])

    def trainNetwork(self):
        if len(self.replayBuffer) > self.batchReplayBufferSize:
            randomSampleBatch = random.sample(self.replayBuffer, self.batchReplayBufferSize)
            currentStateBatch = np.zeros((self.batchReplayBufferSize, self.stateDimension))
            nextStateBatch = np.zeros((self.batchReplayBufferSize, self.stateDimension))
            for index, tupleS in enumerate(randomSampleBatch):
                currentStateBatch[index, :] = tupleS[0]
                nextStateBatch[index, :] = tupleS[3]
            QnextStateTargetNetwork = self.targetNetwork.predict(nextStateBatch)
            QcurrentStateMainNetwork = self.mainNetwork.predict(currentStateBatch)
            inputNetwork = currentStateBatch
            outputNetwork = np.zeros((self.batchReplayBufferSize, self.actionDimension))
            self.actionsAppend = []
            for index, (currentState, action, reward, nextState, terminated) in enumerate(randomSampleBatch):
                if terminated:
                    y = reward
                else:
                    y = reward + self.gamma * np.max(QnextStateTargetNetwork[index])
                self.actionsAppend.append(action)
                outputNetwork[index] = QcurrentStateMainNetwork[index]
                outputNetwork[index, action] = y

            batch_size = min(32, self.batchReplayBufferSize)
            y_true = np.hstack((outputNetwork, np.array(self.actionsAppend).reshape(-1, 1)))
            self.mainNetwork.fit(inputNetwork, y_true, batch_size=batch_size, verbose=1, epochs=1)
            self.counterUpdateTargetNetwork += 1
            if self.counterUpdateTargetNetwork > (self.updateTargetNetworkPeriod - 1):
                self.targetNetwork.set_weights(self.mainNetwork.get_weights())
                print("Target network updated!")
                self.counterUpdateTargetNetwork = 0

    def print_optimal_devices(self):
        print(f"Best total reward: {self.max_reward}")
        print("Optimal set of clients for each step in the best episode:")
        for step, action in enumerate(self.best_actions):
            print(f"Step {step + 1}: Selected clients: {action}")


# Create the custom MEC environment
num_clients = 10
F, E_max, R = 10, 10, 10  # Max resources
alpha_n, alpha_e, alpha_L = 1.0, 1.0, 1.0  # Reward coefficients

env = MECEnv(num_clients, F, E_max, R, alpha_n, alpha_e, alpha_L)

# Define the parameters for the Deep Q-Learning agent
gamma = 0.99
epsilon = 1.0
number_episodes = 100

# Create the DQN agent
agent = DeepQLearning(env, gamma, epsilon, number_episodes)

# Train the agent
agent.trainingEpisodes()

# Print the optimal set of devices
agent.print_optimal_devices()